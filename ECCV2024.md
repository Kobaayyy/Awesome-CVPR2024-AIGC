# Awesome-ECCV2024-AIGC[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A Collection of Papers and Codes for ECCV2024 AIGC

**整理汇总下今年ECCV AIGC相关的论文和代码，具体如下。**

**欢迎star，fork和PR~**

**Please feel free to star, fork or PR if helpful~**

# **参考或转载请注明出处**

ECCV2024官网：https://eccv.ecva.net/

ECCV接收论文列表：

ECCV完整论文库：

开会时间：2024年9月29日-10月4日

论文接收公布时间：2024年

**【Contents】**

- [1.图像生成(Image Generation/Image Synthesis)](#1.图像生成)
- [2.图像编辑（Image Editing)](#2.图像编辑)
- [3.视频生成(Video Generation/Image Synthesis)](#3.视频生成)
- [4.视频编辑(Video Editing)](#4.视频编辑)
- [5.3D生成(3D Generation/3D Synthesis)](#5.3D生成)
- [6.3D编辑(3D Editing)](#6.3D编辑)
- [7.多模态大语言模型(Multi-Modal Large Language Model)](#7.大语言模型)
- [8.其他多任务(Others)](#8.其他)

<a name="1.图像生成"></a>

# 1.图像生成(Image Generation/Image Synthesis)

### Accelerating Diffusion Sampling with Optimized Time Steps

- Paper: https://arxiv.org/abs/2402.17376
- Code: https://github.com/scxue/DM-NonUniform

### AID-AppEAL: Automatic Image Dataset and Algorithm for Content Appeal Enhancement and Assessment Labeling

- Paper: https://arxiv.org/abs/2407.05546v1
- Code: https://github.com/SherryXTChen/AID-Appeal
  
### AnyControl: Create Your Artwork with Versatile Control on Text-to-Image Generation

- Paper: https://arxiv.org/abs/2406.18958
- Code: https://github.com/open-mmlab/AnyControl
  
### A Watermark-Conditioned Diffusion Model for IP Protection

- Paper: 
- Code: https://github.com/rmin2000/WaDiff
  
### BeyondScene: Higher-Resolution Human-Centric Scene Generation With Pretrained Diffusion

- Paper: https://arxiv.org/abs/2404.04544
- Code: https://github.com/gwang-kim/BeyondScene

### ColorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement

- Paper: https://arxiv.org/abs/2407.07197
- Code: https://github.com/moatifbutt/color-peel
  
### ComFusion: Personalized Subject Generation in Multiple Specific Scenes From Single Image

- Paper: https://arxiv.org/abs/2402.11849
- Code:

### ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction

- Paper: https://arxiv.org/abs/2407.07077
- Code: https://github.com/haoosz/ConceptExpress
  
### ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback

- Paper: https://arxiv.org/abs/2404.07987
- Code: https://github.com/liming-ai/ControlNet_Plus_Plus
  
### Data Augmentation for Saliency Prediction via Latent Diffusion

- Paper: 
- Code: https://github.com/IVRL/AugSal
  
### Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics

- Paper: https://arxiv.org/abs/2310.17316
- Code: https://github.com/EnVision-Research/Defect_Spectrum

### DiffFAS: Face Anti-Spoofing via Generative Diffusion Models

- Paper: 
- Code: https://github.com/murphytju/DiffFAS
  
### DiffiT: Diffusion Vision Transformers for Image Generation

- Paper: https://arxiv.org/abs/2312.02139
- Code: https://github.com/NVlabs/DiffiT

### Enhancing Diffusion Models with Text-Encoder Reinforcement Learning

- Paper: https://arxiv.org/abs/2311.15657
- Code: https://github.com/chaofengc/TexForce

### Getting it Right: Improving Spatial Consistency in Text-to-Image Models

- Paper: https://arxiv.org/abs/2404.01197
- Code: https://github.com/SPRIGHT-T2I/SPRIGHT
  
### Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering

- Paper: https://arxiv.org/abs/2403.09622
- Code: https://github.com/AIGText/Glyph-ByT5

### HiDiffusion: Unlocking Higher-Resolution Creativity and Efficiency in Pretrained Diffusion Models

- Paper: https://arxiv.org/abs/2311.17528
- Code: https://github.com/megvii-research/HiDiffusion
  
### HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance

- Paper: https://arxiv.org/abs/2407.06937
- Code: https://github.com/Enderfga/HumanRefiner
  
### Large-scale Reinforcement Learning for Diffusion Models

- Paper: https://arxiv.org/abs/2401.12244
- Code: https://github.com/pinterest/atg-research/tree/main/joint-rl-diffusion

### Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation

- Paper: https://arxiv.org/abs/2402.10491
- Code: https://github.com/GuoLanqing/Self-Cascade
  
### MasterWeaver: Taming Editability and Identity for Personalized Text-to-Image Generation

- Paper: https://arxiv.org/abs/2405.05806
- Code: https://github.com/csyxwei/MasterWeaver
  
### Memory-Efficient Fine-Tuning for Quantized Diffusion Model

- Paper: 
- Code: https://github.com/ugonfor/TuneQDM

### Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models

- Paper: https://arxiv.org/abs/2404.07389
- Code: https://github.com/YasminZhang/EBAMA
  
### OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models

- Paper: https://arxiv.org/abs/2403.10983
- Code: https://github.com/kongzhecn/OMG

### PartCraft: Crafting Creative Objects by Parts

- Paper: https://arxiv.org/abs/2311.15477
- Code: https://github.com/kamwoh/partcraft
  
### Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning

- Paper: 
- Code: https://github.com/wfanyue/DPG-T2I-Personalization
  
### Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts

- Paper: https://arxiv.org/abs/2403.09176
- Code: https://github.com/byeongjun-park/Switch-DiT

### StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion

- Paper: https://arxiv.org/abs/2404.05979
- Code: https://github.com/tobran/StoryImager

### Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction

- Paper: https://arxiv.org/abs/2404.02905
- Code: https://github.com/yuhuUSTC/Debias
  
### ZigMa: A DiT-Style Mamba-based Diffusion Model

- Paper: https://arxiv.org/abs/2403.13802
- Code: https://github.com/CompVis/zigma



<a name="2.图像编辑"></a>

# 2.图像编辑(Image Editing)


### A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting

- Paper: https://arxiv.org/abs/2312.03594
- Code: https://github.com/open-mmlab/PowerPaint

### BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion

- Paper: https://arxiv.org/abs/2403.06976
- Code: https://github.com/TencentARC/BrushNet

### Efficient 3D-Aware Facial Image Editing via Attribute-Specific Prompt Learning

- Paper: https://arxiv.org/abs/2406.04413
- Code: https://github.com/VIROBO-15/Efficient-3D-Aware-Facial-Image-Editing

### Every Pixel Has its Moments: Ultra-High-Resolution Unpaired Image-to-Image Translation via Dense Normalization

- Paper: https://arxiv.org/abs/2407.04245
- Code: https://github.com/Kaminyou/Dense-Normalization
  
### FlexiEdit: Frequency-Aware Latent Refinement for Enhanced Non-Rigid Editing

- Paper: 
- Code: https://github.com/kookie12/FlexiEdit

### Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models

- Paper: https://arxiv.org/abs/2403.11105
- Code: https://github.com/leeruibin/SPDInv
  
### StableDrag: Stable Dragging for Point-based Image Editing

- Paper: https://arxiv.org/abs/2403.04437
- Code: 
  
### TinyBeauty: Toward Tiny and High-quality Facial Makeup with Data Amplify Learning

- Paper: https://arxiv.org/abs/2403.15033
- Code: https://github.com/TinyBeauty/TinyBeauty


<a name="3.视频生成"></a>

# 3.视频生成(Video Generation/Video Synthesis)

### Audio-Synchronized Visual Animation

- Paper: https://arxiv.org/abs/2403.05659
- Code: https://github.com/lzhangbj/ASVA

### Dyadic Interaction Modeling for Social Behavior Generation

- Paper: https://arxiv.org/abs/2403.09069
- Code: https://github.com/Boese0601/Dyadic-Interaction-Modeling

### DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors

- Paper: https://arxiv.org/abs/2310.12190
- Code: https://github.com/Doubiiu/DynamiCrafter
  
### EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis

- Paper: https://arxiv.org/abs/2404.01647
- Code: https://github.com/tanshuai0219/EDTalk

### FreeInit : Bridging Initialization Gap in Video Diffusion Models

- Paper: https://arxiv.org/abs/2312.07537
- Code: https://github.com/TianxingWu/FreeInit
  
### MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model

- Paper: https://arxiv.org/abs/2405.20222
- Code: https://github.com/MyNiuuu/MOFA-Video

### Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models

- Paper: 
- Code: https://github.com/yangqy1110/NC-SDEdit
  
### VEnhancer: Generative Space-Time Enhancement for Video Generation

- Paper: 
- Code: https://github.com/Vchitect/VEnhancer
  
### ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to Video

- Paper: https://arxiv.org/abs/2310.01324
- Code: 


  
<a name="4.视频编辑"></a>

# 4.视频编辑(Video Editing)

### Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation

- Paper: https://arxiv.org/abs/2403.13745
- Code: https://github.com/G-U-N/Be-Your-Outpainter
  
### DragAnything: Motion Control for Anything using Entity Representation

- Paper: https://arxiv.org/abs/2403.07420
- Code: https://github.com/showlab/DragAnything




<a name="5.3D生成"></a>

# 5.3D生成(3D Generation/3D Synthesis)

### EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion

- Paper: https://arxiv.org/abs/2405.00915
- Code: https://github.com/ymxlzgy/echoscene

### GenerateCT: Text-Conditional Generation of 3D Chest CT Volumes

- Paper: https://arxiv.org/abs/2405.00915
- Code: https://github.com/ibrahimethemhamamci/GenerateCT

### GVGEN:Text-to-3D Generation with Volumetric Representation

- Paper: 
- Code: https://github.com/SOTAMak1r/GVGEN

### HiFi-123: Towards High-fidelity One Image to 3D Content Generation

- Paper: https://github.com/AILab-CVC/HiFi-123
- Code: https://arxiv.org/abs/2310.06744

### KMTalk: Speech-Driven 3D Facial Animationwith Key Motion Embedding 

- Paper: 
- Code: https://github.com/ffxzh/KMTalk
  
### MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model

- Paper: https://arxiv.org/abs/2404.19759
- Code: https://github.com/Dai-Wenxun/MotionLCM
  
### Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM

- Paper: https://arxiv.org/abs/2403.07487
- Code: https://github.com/steve-zeyu-zhang/MotionMamba
  
### ParCo: Part-Coordinating Text-to-Motion Synthesis

- Paper: https://arxiv.org/abs/2403.18512
- Code: https://github.com/qrzou/ParCo

### ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score Distillation

- Paper: https://arxiv.org/abs/2407.02040
- Code: https://github.com/theEricMa/ScaleDreamer

### Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using Diffusion Models

- Paper: https://arxiv.org/abs/2311.17050
- Code: https://github.com/Yzmblog/SurfD

### VividDreamer: Invariant Score Distillation For Hyper-Realistic Text-to-3D Generation

- Paper: 
- Code: https://github.com/SupstarZh/VividDreamer
  

<a name="6.3D编辑"></a>

# 6.3D编辑(3D Editing)

### Chat Edit 3D: Interactive 3D Scene Editing via Text Prompts

- Paper: https://arxiv.org/abs/2407.06842
- Code: https://github.com/Fangkang515/CE3D
  
### Gaussian Grouping: Segment and Edit Anything in 3D Scenes

- Paper: https://arxiv.org/abs/2312.00732
- Code: https://github.com/lkeab/gaussian-grouping
  
### SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer

- Paper: https://arxiv.org/abs/2403.18512
- Code: https://github.com/JarrentWu1031/SC4D

### Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing

- Paper: https://arxiv.org/abs/2403.10050
- Code: https://github.com/slothfulxtx/Texture-GS


<a name="7.大语言模型"></a>

# 7.多模态大语言模型(Multi-Modal Large Language Models)

### About Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model

- Paper: 
- Code: https://github.com/ChaduCheng/TypoDeceptions
  
### AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting

- Paper: https://arxiv.org/abs/2403.09513
- Code: https://github.com/SaFoLab-WISC/AdaShield
  
### An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models

- Paper: https://arxiv.org/abs/2403.06764
- Code: https://github.com/pkunlp-icler/FastV

### ControlCap: Controllable Region-level Captioning

- Paper: https://arxiv.org/abs/2401.17910
- Code: https://github.com/callsys/ControlCap

### DreamLIP: Language-Image Pre-training with Long Captions

- Paper: https://arxiv.org/abs/2403.17007
- Code: https://github.com/zyf0619sjtu/DreamLIP
  
### DriveLM: Driving with Graph Visual Question Answering

- Paper: https://arxiv.org/abs/2312.14150
- Code: https://github.com/OpenDriveLab/DriveLM

### Elysium: Exploring Object-level Perception in Videos via MLLM

- Paper: https://arxiv.org/abs/2403.16558
- Code: https://github.com/Hon-Wong/Elysium

### Empowering Multimodal Large Language Model as a Powerful Data Generator

- Paper: 
- Code: https://github.com/zhaohengyuan1/Genixer

### InternVideo: Video Foundation Models for Multimodal Understanding

- Paper: https://arxiv.org/abs/2212.03191
- Code: https://github.com/OpenGVLab/InternVideo
  
### Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks

- Paper: 
- Code: https://github.com/tingyu215/Routing_VLPEFT
  
### GiT: Towards Generalist Vision Transformer through Universal Language Interface

- Paper: https://arxiv.org/abs/2403.09394
- Code: https://github.com/Haiyang-W/GiT

### How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs

- Paper: https://arxiv.org/abs/2311.17600
- Code: https://github.com/UCSC-VLAA/vllm-safety-benchmark

### LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models

- Paper: 
- Code: https://github.com/YBZh/LAPT

### LLMGA: Multimodal Large Language Model-based Generation Assistant

- Paper: https://arxiv.org/abs/2311.16500
- Code: https://github.com/dvlab-research/LLMGA
  
### Long-CLIP: Unlocking the Long-Text Capability of CLIP

- Paper: https://arxiv.org/abs/2403.15378
- Code: https://github.com/beichenzbc/Long-CLIP
  
### MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?

- Paper: https://arxiv.org/abs/2403.14624
- Code: https://github.com/ZrrSkywalker/MathVerse

### Merlin:Empowering Multimodal LLMs with Foresight Minds

- Paper: https://arxiv.org/abs/2312.00589
- Code: https://github.com/Ahnsun/merlin

### Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs

- Paper: https://arxiv.org/abs/2403.11755
- Code: https://github.com/jmiemirza/Meta-Prompting
  
### MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models

- Paper: https://arxiv.org/abs/2403.14624
- Code: https://github.com/isXinLiu/MM-SafetyBench

### NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models

- Paper: https://arxiv.org/abs/2305.16986
- Code: https://github.com/GengzeZhou/NavGPT-2

### Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models

- Paper: https://arxiv.org/abs/2404.12139
- Code: https://github.com/Heathcliff-saku/Omniview_Tuning
  
### PointLLM: Empowering Large Language Models to Understand Point Clouds

- Paper: https://arxiv.org/abs/2308.16911
- Code: https://github.com/OpenRobotLab/PointLLM
  
### R2-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations

- Paper: https://arxiv.org/abs/2403.04924
- Code: https://github.com/lxa9867/r2bench
  
### SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation

- Paper: 
- Code: https://github.com/AI-Application-and-Integration-Lab/SAM4MLLM

### SDPT: Synchronous Dual Prompt Tuning for Fusion-based Visual-Language Pre-trained Models

- Paper: 
- Code: https://github.com/wuyongjianCODE/SDPT
  
### ShareGPT4V: Improving Large Multi-Modal Models with Better Captions

- Paper: https://arxiv.org/abs/2311.12793
- Code: https://github.com/ShareGPT4Omni/ShareGPT4V

### ST-LLM: Large Language Models Are Effective Temporal Learners

- Paper: https://arxiv.org/abs/2404.00308
- Code: https://github.com/TencentARC/ST-LLM
  
### TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias

- Paper: https://arxiv.org/abs/2404.00384
- Code: https://github.com/shjo-april/TTD
  
### UniIR: Training and Benchmarking Universal Multimodal Information Retrievers

- Paper: https://arxiv.org/abs/2311.17136
- Code: https://github.com/TIGER-AI-Lab/UniIR

### Vary: Scaling Up the Vision Vocabulary of Large Vision Language Models

- Paper: https://arxiv.org/abs/2312.06109
- Code: https://github.com/Ucas-HaoranWei/Vary

  
<a name="8.其他"></a>

# 8.其他任务(Others)





<font color=red size=5>持续更新~</font>

# 参考


# 相关整理

- [Awesome-CVPR2024-AIGC](https://github.com/Kobaayyy/Awesome-CVPR2024-AIGC/blob/main/CVPR2024.md)
- [Awesome-AIGC-Research-Groups](https://github.com/Kobaayyy/Awesome-AIGC-Research-Groups)
- [Awesome-Low-Level-Vision-Research-Groups](https://github.com/Kobaayyy/Awesome-Low-Level-Vision-Research-Groups)
- [Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision)
- [Awesome-ECCV2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-ECCV2020-Low-Level-Vision)
